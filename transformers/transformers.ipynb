{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Implementation and Examples\n",
    "\n",
    "This notebook provides a comprehensive guide to understanding and implementing transformers from scratch, as well as using pre-trained models.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Attention Mechanism](#attention)\n",
    "3. [Multi-Head Attention](#multihead)\n",
    "4. [Positional Encoding](#positional)\n",
    "5. [Transformer Building Blocks](#blocks)\n",
    "6. [Using Pre-trained Models](#pretrained)\n",
    "7. [Fine-tuning Example](#finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention <a name=\"attention\"></a>\n",
    "\n",
    "The fundamental building block of transformers:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor of shape (batch_size, seq_len, d_k)\n",
    "        key: Key tensor of shape (batch_size, seq_len, d_k)\n",
    "        value: Value tensor of shape (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention weights for visualization\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute output\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_k = 2, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.title('Attention Weights Visualization')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention <a name=\"multihead\"></a>\n",
    "\n",
    "Multiple attention heads allow the model to attend to different aspects of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k)\"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Example usage\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Multi-head attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding <a name=\"positional\"></a>\n",
    "\n",
    "Since transformers don't have inherent notion of order, we add positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Example and visualization\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Visualize positional encodings\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pos_encoding.pe[0, :50, :].numpy(), cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Building Blocks <a name=\"blocks\"></a>\n",
    "\n",
    "Complete encoder and decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(f\"Encoder layer output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Pre-trained Models <a name=\"pretrained\"></a>\n",
    "\n",
    "Let's use Hugging Face transformers library to load and use pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"Transformers are powerful neural network architectures.\",\n",
    "    \"Attention mechanisms enable parallel processing of sequences.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "# Extract embeddings\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(f\"Output shape: {last_hidden_states.shape}\")\n",
    "print(f\"Shape: (batch_size={last_hidden_states.shape[0]}, seq_len={last_hidden_states.shape[1]}, hidden_size={last_hidden_states.shape[2]})\")\n",
    "\n",
    "# Extract CLS token embeddings (sentence representations)\n",
    "cls_embeddings = last_hidden_states[:, 0, :]\n",
    "print(f\"\\nCLS embeddings shape: {cls_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning Example <a name=\"finetuning\"></a>\n",
    "\n",
    "Example of how to fine-tune a pre-trained transformer for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "# Create a classification model\n",
    "num_labels = 2  # Binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Example training data\n",
    "train_texts = [\n",
    "    \"This movie was fantastic!\",\n",
    "    \"I didn't enjoy this film at all.\",\n",
    "    \"Great storyline and acting.\",\n",
    "    \"Boring and predictable.\"\n",
    "]\n",
    "train_labels = [1, 0, 1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Create simple training loop (simplified for demonstration)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(**train_encodings, labels=torch.tensor(train_labels))\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "test_text = [\"This is an amazing movie!\"]\n",
    "test_encoding = tokenizer(test_text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encoding)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "print(f\"\\nPrediction: {'Positive' if predictions.item() == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Attention Mechanism**: The core building block of transformers\n",
    "2. **Multi-Head Attention**: Parallel attention mechanisms for richer representations\n",
    "3. **Positional Encoding**: Adding sequence order information\n",
    "4. **Transformer Layers**: Complete encoder/decoder implementations\n",
    "5. **Pre-trained Models**: Using BERT and other models from Hugging Face\n",
    "6. **Fine-tuning**: Adapting pre-trained models for specific tasks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different architectures (GPT, T5, etc.)\n",
    "- Try fine-tuning on your own datasets\n",
    "- Explore Vision Transformers for image tasks\n",
    "- Study optimization techniques and training strategies\n",
    "- Implement efficient attention variants (e.g., Linear Attention, Performer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
